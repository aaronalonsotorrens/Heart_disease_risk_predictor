{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8df5593",
   "metadata": {},
   "source": [
    "# üß™ Notebook 6 ‚Äî Model Deployment & Inference\n",
    "\n",
    "**Objective:**  \n",
    "\n",
    "Prepare the final, tuned model for deployment. This notebook will:\n",
    " \n",
    "- Load the best model(s) from Notebook 5.  \n",
    "  \n",
    "- Build preprocessing + prediction pipeline.  \n",
    "  \n",
    "- Create functions for inference on new data.  \n",
    "  \n",
    "- Test predictions with sample inputs.  \n",
    "  \n",
    "- Visualize feature importance for interpretability.  \n",
    "  \n",
    "- Save the pipeline for deployment.\n",
    " \n",
    " \n",
    "This ensures that new data can be fed into the model consistently and predictions are reproducible.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caa4c37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4 tuned models from models/tuned_v2\n"
     ]
    }
   ],
   "source": [
    "## 6.1 Load Tuned Models & Training Schema\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load tuned models\n",
    "tuned_dir = \"models/tuned_v2\"\n",
    "tuned_models = {\n",
    "    \"Logistic Regression\": joblib.load(f\"{tuned_dir}/best_logistic_regression_pipeline.pkl\"),\n",
    "    \"Random Forest\": joblib.load(f\"{tuned_dir}/best_random_forest_pipeline.pkl\"),\n",
    "    \"XGBoost\": joblib.load(f\"{tuned_dir}/best_xgboost_pipeline.pkl\"),\n",
    "    \"LightGBM\": joblib.load(f\"{tuned_dir}/best_lightgbm_pipeline.pkl\"),\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(tuned_models)} tuned models from {tuned_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f39acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_expected_features(model_pipeline):\n",
    "    \"\"\"\n",
    "    Extract original feature names from the pipeline.\n",
    "    Falls back to None if unavailable.\n",
    "    \"\"\"\n",
    "    if \"preprocessor\" in model_pipeline.named_steps:\n",
    "        preprocessor = model_pipeline.named_steps[\"preprocessor\"]\n",
    "        if hasattr(preprocessor, \"feature_names_in_\"):\n",
    "            return list(preprocessor.feature_names_in_)\n",
    "    return None\n",
    "\n",
    "def align_input(sample_data: pd.DataFrame, expected_features):\n",
    "    \"\"\"\n",
    "    Align new input to match training schema.\n",
    "    Missing cols -> filled with 0\n",
    "    Extra cols -> dropped\n",
    "    \"\"\"\n",
    "    return sample_data.reindex(columns=expected_features, fill_value=0)\n",
    "\n",
    "def predict_pipeline(model_pipeline, new_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Run inference using a preprocessing + model pipeline.\n",
    "    Returns predicted class and probability.\n",
    "    \"\"\"\n",
    "    pred_class = model_pipeline.predict(new_data)\n",
    "    pred_proba = model_pipeline.predict_proba(new_data)[:, 1]\n",
    "    return pred_class, pred_proba\n",
    "\n",
    "def predict_from_dict(model_pipeline, patient_dict: dict):\n",
    "    \"\"\"\n",
    "    Convenience wrapper: pass patient record as dict.\n",
    "    Auto-aligns to training schema.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame([patient_dict])\n",
    "    expected_features = get_expected_features(model_pipeline)\n",
    "    if expected_features is not None:\n",
    "        df = align_input(df, expected_features)\n",
    "    return predict_pipeline(model_pipeline, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41359817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Logistic Regression\n",
      "Prediction: 0\n",
      "Probability: 0.021\n",
      "\n",
      "üîπ Random Forest\n",
      "Prediction: 0\n",
      "Probability: 0.417\n",
      "\n",
      "üîπ XGBoost\n",
      "Prediction: 0\n",
      "Probability: 0.354\n",
      "\n",
      "üîπ LightGBM\n",
      "Prediction: 0\n",
      "Probability: 0.221\n"
     ]
    }
   ],
   "source": [
    "# Example raw patient record (simple input)\n",
    "sample_patient = {\n",
    "    \"age\": 55,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 3,\n",
    "    \"trestbps\": 140,\n",
    "    \"chol\": 220,\n",
    "    \"fbs\": 0,\n",
    "    \"restecg\": 1,\n",
    "    \"thalch\": 150,\n",
    "    \"exang\": 0,\n",
    "    \"oldpeak\": 1.5,\n",
    "}\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    pred_class, pred_proba = predict_from_dict(model, sample_patient)\n",
    "    print(f\"\\nüîπ {name}\")\n",
    "    print(\"Prediction:\", int(pred_class[0]))\n",
    "    print(\"Probability:\", round(pred_proba[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "127e95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping Random Forest: mismatch between features and importances\n",
      "‚ö†Ô∏è Skipping XGBoost: mismatch between features and importances\n",
      "‚ö†Ô∏è Skipping LightGBM: mismatch between features and importances\n"
     ]
    }
   ],
   "source": [
    "# ## 6.4 Feature Importance (Optional)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    final_estimator = model[-1]  # last step in pipeline\n",
    "    \n",
    "    if hasattr(final_estimator, \"feature_importances_\"):\n",
    "        importances = final_estimator.feature_importances_\n",
    "        expected_features = get_expected_features(model)\n",
    "        feature_names = expected_features if expected_features is not None else np.arange(len(importances))\n",
    "        \n",
    "        if len(importances) == len(feature_names):\n",
    "            fi_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "            fi_df = fi_df.sort_values(\"Importance\", ascending=False)\n",
    "            \n",
    "            plt.figure(figsize=(10,6))\n",
    "            sns.barplot(x=\"Importance\", y=\"Feature\", data=fi_df, palette=\"viridis\")\n",
    "            plt.title(f\"{name} - Feature Importances\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {name}: mismatch between features and importances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35fba473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final tuned pipelines saved for deployment in 'models/deployment'\n"
     ]
    }
   ],
   "source": [
    "## 6.5 Save Pipelines for Deployment\n",
    "os.makedirs(\"models/deployment\", exist_ok=True)\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    file_name = f\"models/deployment/final_{name.replace(' ', '_').lower()}_pipeline.pkl\"\n",
    "    joblib.dump(model, file_name)\n",
    "\n",
    "print(\"‚úÖ Final tuned pipelines saved for deployment in 'models/deployment'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "692eeae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6.6 Enhanced Inference with Risk Bands & Explanations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def risk_band(prob):\n",
    "    \"\"\"\n",
    "    Translate probability into risk band.\n",
    "    \"\"\"\n",
    "    if prob < 0.2:\n",
    "        return \"Low\"\n",
    "    elif prob < 0.5:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "def enhanced_predict(model_pipeline, new_data: pd.DataFrame, top_n=3):\n",
    "    \"\"\"\n",
    "    Enhanced prediction: class, probability, risk band,\n",
    "    top contributing features, and recommendation.\n",
    "    \"\"\"\n",
    "    # Align new data to match training schema\n",
    "    expected_features = get_expected_features(model_pipeline)\n",
    "    if expected_features is not None:\n",
    "        new_data = align_input(new_data, expected_features)\n",
    "\n",
    "    # Predict class + probability\n",
    "    pred_class = int(model_pipeline.predict(new_data)[0])\n",
    "    pred_proba = float(model_pipeline.predict_proba(new_data)[:, 1][0])\n",
    "    band = risk_band(pred_proba)\n",
    "\n",
    "    # Recommendation\n",
    "    recommendation = (\n",
    "        \"Maintain healthy lifestyle\" if band == \"Low\"\n",
    "        else \"Recommend further testing\"\n",
    "    )\n",
    "\n",
    "    # Get feature names from preprocessor\n",
    "    preprocessor = model_pipeline.named_steps.get(\"preprocessor\")\n",
    "    feature_names = (\n",
    "        preprocessor.get_feature_names_out()\n",
    "        if hasattr(preprocessor, \"get_feature_names_out\")\n",
    "        else [f\"f{i}\" for i in range(new_data.shape[1])]\n",
    "    )\n",
    "\n",
    "    contributions = None\n",
    "    # Logistic Regression ‚Üí use coefficients √ó scaled input\n",
    "    if \"log_reg\" in model_pipeline.named_steps:\n",
    "        model = model_pipeline.named_steps[\"log_reg\"]\n",
    "        X_scaled = preprocessor.transform(new_data)\n",
    "        contributions = (X_scaled.toarray() if hasattr(X_scaled, \"toarray\") else X_scaled)[0] * model.coef_[0]\n",
    "\n",
    "    # Tree-based models ‚Üí approximate with feature importances\n",
    "    elif any(k in model_pipeline.named_steps for k in [\"rf\", \"xgb\", \"lgbm\"]):\n",
    "        model = list(model_pipeline.named_steps.values())[-1]\n",
    "        importances = model.feature_importances_\n",
    "        contributions = importances * pred_proba\n",
    "\n",
    "    # Build top contributions dataframe\n",
    "    if contributions is not None and len(contributions) == len(feature_names):\n",
    "        contrib_df = pd.DataFrame({\n",
    "            \"Feature\": feature_names,\n",
    "            \"Contribution\": contributions\n",
    "        }).reindex(feature_names)\n",
    "        contrib_df = contrib_df.reindex(contrib_df.Contribution.abs().sort_values(ascending=False).index)\n",
    "        top_contrib = contrib_df.head(top_n)\n",
    "    else:\n",
    "        top_contrib = pd.DataFrame(columns=[\"Feature\", \"Contribution\"])\n",
    "\n",
    "    return {\n",
    "        \"Prediction\": pred_class,\n",
    "        \"Probability\": round(pred_proba, 3),\n",
    "        \"Risk Band\": band,\n",
    "        \"Recommendation\": recommendation,\n",
    "        \"Top Contributions\": top_contrib\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97b9ad9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Logistic Regression Enhanced Prediction\n",
      "Prediction: 0\n",
      "Probability: 0.059\n",
      "Risk Band: Low\n",
      "Recommendation: Maintain healthy lifestyle\n",
      "Top Contributions:\n",
      "               Feature  Contribution\n",
      "num__id           NaN           NaN\n",
      "num__age          NaN           NaN\n",
      "num__trestbps     NaN           NaN\n",
      "\n",
      "üîπ Random Forest Enhanced Prediction\n",
      "Prediction: 0\n",
      "Probability: 0.491\n",
      "Risk Band: Medium\n",
      "Recommendation: Recommend further testing\n",
      "Top Contributions:\n",
      "               Feature  Contribution\n",
      "num__id           NaN           NaN\n",
      "num__age          NaN           NaN\n",
      "num__trestbps     NaN           NaN\n",
      "\n",
      "üîπ XGBoost Enhanced Prediction\n",
      "Prediction: 1\n",
      "Probability: 0.514\n",
      "Risk Band: High\n",
      "Recommendation: Recommend further testing\n",
      "Top Contributions:\n",
      "               Feature  Contribution\n",
      "num__id           NaN           NaN\n",
      "num__age          NaN           NaN\n",
      "num__trestbps     NaN           NaN\n",
      "\n",
      "üîπ LightGBM Enhanced Prediction\n",
      "Prediction: 0\n",
      "Probability: 0.364\n",
      "Risk Band: Medium\n",
      "Recommendation: Recommend further testing\n",
      "Top Contributions:\n",
      "               Feature  Contribution\n",
      "num__id           NaN           NaN\n",
      "num__age          NaN           NaN\n",
      "num__trestbps     NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "## 6.7 Test Enhanced Inference with Sample Data\n",
    "\n",
    "# Example patient\n",
    "sample_data = pd.DataFrame({\n",
    "    \"age\": [68],\n",
    "    \"sex\": [1],\n",
    "    \"cp\": [4],\n",
    "    \"trestbps\": [180],\n",
    "    \"chol\": [300],\n",
    "    \"fbs\": [1],\n",
    "    \"restecg\": [2],\n",
    "    \"thalch\": [120],\n",
    "    \"exang\": [1],\n",
    "    \"oldpeak\": [3.0],\n",
    "})\n",
    "\n",
    "for name, model in tuned_models.items():\n",
    "    result = enhanced_predict(model, sample_data)\n",
    "    print(f\"\\nüîπ {name} Enhanced Prediction\")\n",
    "    print(\"Prediction:\", result[\"Prediction\"])\n",
    "    print(\"Probability:\", result[\"Probability\"])\n",
    "    print(\"Risk Band:\", result[\"Risk Band\"])\n",
    "    print(\"Recommendation:\", result[\"Recommendation\"])\n",
    "    print(\"Top Contributions:\\n\", result[\"Top Contributions\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
